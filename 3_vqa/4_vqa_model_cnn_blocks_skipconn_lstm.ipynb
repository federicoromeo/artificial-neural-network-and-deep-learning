{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AicZNG52-rVX"
   },
   "source": [
    "# CNN from scratch + skip connections and LSTM RNN\n",
    "In this first model for Visual Question Answering problem, we built from scratch our Convolutional Neural Network, by creating some usual Convolutional blocks and adding to them some skip connection, in order to wxtract feature of dirrefent sizes. For Recurrent Neural Network part, we used a Long Short Term Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1609882956180,
     "user": {
      "displayName": "Federico Romeo",
      "photoUrl": "https://lh6.googleusercontent.com/-t7-ZYUoSe3w/AAAAAAAAAAI/AAAAAAAAA8c/9aa1pe6GlnY/s64/photo.jpg",
      "userId": "18144303458962689975"
     },
     "user_tz": -60
    },
    "id": "FiLwi7HcT6Zm"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "executionInfo": {
     "elapsed": 2423,
     "status": "ok",
     "timestamp": 1609882957784,
     "user": {
      "displayName": "Federico Romeo",
      "photoUrl": "https://lh6.googleusercontent.com/-t7-ZYUoSe3w/AAAAAAAAAAI/AAAAAAAAA8c/9aa1pe6GlnY/s64/photo.jpg",
      "userId": "18144303458962689975"
     },
     "user_tz": -60
    },
    "id": "KJqHEF3eT6Zn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations, this let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# img shape\n",
    "img_h = 400\n",
    "img_w = 700\n",
    "\n",
    "num_classes = 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we try to get the best partitioning of category by further attempts (explained in notebook \"1_data_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 212152,
     "status": "ok",
     "timestamp": 1609883167551,
     "user": {
      "displayName": "Federico Romeo",
      "photoUrl": "https://lh6.googleusercontent.com/-t7-ZYUoSe3w/AAAAAAAAAAI/AAAAAAAAA8c/9aa1pe6GlnY/s64/photo.jpg",
      "userId": "18144303458962689975"
     },
     "user_tz": -60
    },
    "id": "eJRYsmsm-rV5"
   },
   "outputs": [],
   "source": [
    "yes_no_prefixes   = [\"do\",\"does\",\"is\",\"are\",\"was\",\"were\",\"have\",\"might\",\"could\",\"can\",\"has\",\"will\",\"did\",\"would\",\"should\",\"if\"]\n",
    "counting_prefixes = [\"how\"]\n",
    "\n",
    "# Predict the category from a question\n",
    "\n",
    "def predict_category(question):\n",
    "    \n",
    "    if list(filter(question.lower().startswith, yes_no_prefixes)) != []:\n",
    "         return \"yes_no\" \n",
    "    elif list(filter(question.lower().startswith, counting_prefixes)) != [] or \"number\" in question.lower() or \"how many\" in question.lower():\n",
    "        return \"counting\"\n",
    "    \n",
    "    else:\n",
    "        return \"other\"\n",
    "    \n",
    "# Get the real category from the associated answer label\n",
    "\n",
    "def real_category(label):\n",
    "    \n",
    "    if label in [57,33]:\n",
    "        return \"yes_no\" \n",
    "    \n",
    "    elif label in [0,1,2,3,4,5]:\n",
    "        return \"counting\"\n",
    "    \n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAIMIdbNT6Zo"
   },
   "source": [
    "### Question-Answer Training Dataset\n",
    "Gather questions and answers from json, and then compute predictions on category (explained in notebook \"1_data_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "executionInfo": {
     "elapsed": 212957,
     "status": "ok",
     "timestamp": 1609883168365,
     "user": {
      "displayName": "Federico Romeo",
      "photoUrl": "https://lh6.googleusercontent.com/-t7-ZYUoSe3w/AAAAAAAAAAI/AAAAAAAAA8c/9aa1pe6GlnY/s64/photo.jpg",
      "userId": "18144303458962689975"
     },
     "user_tz": -60
    },
    "id": "klmuQFqq-rV6",
    "outputId": "e784957b-f86b-4507-a63b-2b47bc57bf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions badly labelled: 622 over 58832\n",
      "In percentage: 1.057 %\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from os import getcwd, listdir\n",
    "from os.path import join\n",
    "import json\n",
    "\n",
    "dataset_dir  = join('../input/anndl-2020-vqa','VQA_Dataset')\n",
    "training_dir = join(dataset_dir, 'Images')\n",
    "\n",
    "labels_dict = { \n",
    "                '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, 'apple': 6,\n",
    "                'baseball': 7, 'bench': 8, 'bike': 9, 'bird': 10, 'black': 11, 'blanket': 12, 'blue': 13,\n",
    "                'bone': 14, 'book': 15, 'boy': 16, 'brown': 17, 'cat': 18, 'chair': 19, 'couch': 20,\n",
    "                'dog': 21, 'floor': 22, 'food': 23, 'football': 24, 'girl': 25, 'grass': 26, 'gray': 27,\n",
    "                'green': 28, 'left': 29, 'log': 30, 'man': 31, 'monkey bars': 32, 'no': 33, 'nothing': 34,\n",
    "                'orange': 35, 'pie': 36, 'plant': 37, 'playing': 38, 'red': 39, 'right': 40, 'rug': 41,\n",
    "                'sandbox': 42, 'sitting': 43, 'sleeping': 44, 'soccer': 45, 'squirrel': 46, 'standing': 47, \n",
    "                'stool': 48, 'sunny': 49, 'table': 50, 'tree': 51, 'watermelon': 52, 'white': 53, 'wine': 54, \n",
    "                'woman': 55, 'yellow': 56, 'yes': 57 \n",
    "              }\n",
    "\n",
    "train_dict  = {}\n",
    "with open(join(dataset_dir,'train_questions_annotations.json')) as json_file:\n",
    "    train_dict = json.load(json_file)\n",
    "\n",
    "# order json by 'image_id'    \n",
    "train_dict = dict(sorted(train_dict.items(), key=lambda x: int(x[1]['image_id'])))\n",
    "\n",
    "questions, labels_answers, answers, answers_id = [], [], [], []\n",
    "bad_label = 0\n",
    "\n",
    "for line in train_dict.items():\n",
    "\n",
    "    questions.append(line[1][\"question\"])\n",
    "    answers.append(line[1][\"answer\"])\n",
    "    labels_answers.append(labels_dict[line[1][\"answer\"]])\n",
    "    \n",
    "    if predict_category(line[1][\"question\"]) != real_category(labels_dict[line[1][\"answer\"]]):\n",
    "        bad_label += 1\n",
    "\n",
    "print(\"Number of questions badly labelled: \" + str(bad_label) + \" over \" + str(58832))\n",
    "print(\"In percentage: \" + str(bad_label/58832*100)[:5] + \" %\")\n",
    "\n",
    "# one-hot encoding for answers\n",
    "for a in answers:\n",
    "    answers_id.append(to_categorical(labels_dict.get(a), num_classes=58))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75sNY9KBT6Zo"
   },
   "source": [
    "## Tokenization\n",
    "Converts question's words to integers, and retrieve maximum question length. It will be used as a parameter for question input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "5ADaD89lT6Zo",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions: 4640\n",
      "Max question length: 21\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_SENTENCES = 40000\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "# QUESTIONS \n",
    "\n",
    "# Create Tokenizer to convert words to integers\n",
    "questions_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "\n",
    "# pass the sentences retrieved from json\n",
    "questions_tokenizer.fit_on_texts(questions)\n",
    "\n",
    "# returns a list of lists of indexes referring to words inside that sentence (max of words)\n",
    "questions_tokenized = questions_tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "# returns a dict with words lowercased in alphabetical order and indexed wrt cardinality\n",
    "questions_wtoi = questions_tokenizer.word_index\n",
    "print('Questions:', len(questions_wtoi))\n",
    "\n",
    "max_question_length = max(len(sentence) for sentence in questions_tokenized)\n",
    "print('Max question length:', max_question_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMHmTPzdT6Zo"
   },
   "source": [
    "### Padding sequences\n",
    "LSTM wants batches of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "SsAMc718T6Zp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions encoder inputs shape: (58832, 21)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad (with 0) to max question length\n",
    "questions_encoded = pad_sequences(questions_tokenized, maxlen=max_question_length)\n",
    "\n",
    "print(\"Questions encoder inputs shape:\", questions_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a 90% training/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52948"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5884"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "52948"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5884"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "52948"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5884"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [int(el[1]['image_id']) for el in train_dict.items()]   \n",
    "split = int(len(filenames)*0.90)  \n",
    "\n",
    "filenames_train = filenames[:split]\n",
    "filenames_valid = filenames[split:]\n",
    "len(filenames_train)\n",
    "len(filenames_valid)\n",
    "\n",
    "questions_encoded_train = questions_encoded[:split]\n",
    "questions_encoded_valid = questions_encoded[split:]\n",
    "len(questions_encoded_train)\n",
    "len(questions_encoded_valid)\n",
    "\n",
    "answers_train = answers_id[:split]\n",
    "answers_valid = answers_id[split:]\n",
    "len(answers_train)\n",
    "len(answers_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset that returns every time a tuple like:  ( [image, question], answer ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "-BEj9f5G1PHQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50  import preprocess_input \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, filenames, questions, answers, preprocessing_function=preprocess_input, out_shape=[img_h,img_w],img_generator=ImageDataGenerator(rescale=1./255 - 0.5)):\n",
    "\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.filenames = filenames\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def __len__(self): return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Read Image\n",
    "        curr_filename = self.filenames[index]\n",
    "        img = Image.open(join(training_dir, str(curr_filename) + '.png')).convert('RGB')\n",
    "\n",
    "        # Resize image\n",
    "        resized_img = img#.resize((img_h, img_w))\n",
    "        img_arr = np.array(resized_img)\n",
    "\n",
    "        return (img_arr, self.questions[index]), self.answers[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_custom_dataset = CustomDataset( filenames=filenames_valid, questions=questions_encoded_valid, answers=answers_valid )\n",
    "train_custom_dataset = CustomDataset( filenames=filenames_train, questions=questions_encoded_train, answers=answers_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally create the train and valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: (((None, 400, 700, 3), (None, 21)), (None, 58)), types: ((tf.uint8, tf.int32), tf.int32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: (((None, 400, 700, 3), (None, 21)), (None, 58)), types: ((tf.uint8, tf.int32), tf.int32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_custom_dataset,\n",
    "                                               output_types=((tf.uint8,tf.int32), tf.int32),\n",
    "                                               output_shapes=(([img_h, img_w, 3],(max_question_length,)), (58,)) )\n",
    "train_dataset = train_dataset.batch(32)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_custom_dataset,\n",
    "                                               output_types=((tf.uint8,tf.int32), tf.int32),\n",
    "                                               output_shapes=(([img_h, img_w, 3],(max_question_length,)), (58,)) )\n",
    "valid_dataset = valid_dataset.batch(32)\n",
    "valid_dataset = valid_dataset.repeat()\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fyx28iuHT6Zp"
   },
   "source": [
    "# Model\n",
    "CNN: Convolutional blocks\n",
    "RNN: lsmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "8IC7MPwGT6Zp"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 16\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=[max_question_length])\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(len(questions_wtoi)+1, EMBEDDING_SIZE, input_length=max_question_length, mask_zero=True)\n",
    "encoder_embedding_out = encoder_embedding_layer(encoder_input)\n",
    "encoder_LSTM = tf.keras.layers.LSTM(units=128, activation='tanh', return_state=False)(encoder_embedding_out)\n",
    "\n",
    "# CNN\n",
    "\n",
    "cnn_input = tf.keras.Input(shape=(img_h, img_w, 3))\n",
    "start_f = 8\n",
    "x1 = tf.keras.layers.Conv2D(start_f, 7, padding='same')(cnn_input)\n",
    "x1 = tf.keras.layers.MaxPooling2D()(x1) # 200 x 350 x 8\n",
    "activation1 = tf.keras.layers.Activation('relu')(x1)\n",
    "start_f = start_f * 2 \n",
    "x1 = tf.keras.layers.Conv2D(start_f, 5, padding='same')(activation1)\n",
    "x1 = tf.keras.layers.MaxPooling2D()(x1) # 100 x 175 x 16\n",
    "activation2 = tf.keras.layers.Activation('relu')(x1)\n",
    "start_f = start_f * 2 \n",
    "x1 = tf.keras.layers.Conv2D(start_f, 3, padding='same')(activation2)\n",
    "x1 = tf.keras.layers.MaxPooling2D()(x1) # 50 x 87.5 x 32\n",
    "x1 = tf.keras.layers.Conv2D(start_f, 3, padding='same')(x1)\n",
    "x1 = tf.keras.layers.MaxPooling2D()(x1) # 25 x 43 x 32\n",
    "x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "\n",
    "flatten1 = tf.keras.layers.Flatten()(x1)\n",
    "dense1 = tf.keras.layers.Dense(units=64, activation='relu')(flatten1)\n",
    "flatten2 = tf.keras.layers.Flatten()(activation1)\n",
    "dense2 = tf.keras.layers.Dense(units=32, activation='relu')(flatten2)\n",
    "flatten3 = tf.keras.layers.Flatten()(activation2)\n",
    "dense3 = tf.keras.layers.Dense(units=32, activation='relu')(flatten2)\n",
    "\n",
    "concatenate = tf.keras.layers.Concatenate()([dense1, dense2, dense3])\n",
    "\n",
    "# Combine CNN and RNN to create the final model\n",
    "\n",
    "merged = tf.keras.layers.Multiply()([encoder_LSTM, concatenate])\n",
    "classes = len(labels_dict)  #58\n",
    "output = tf.keras.layers.Dense(units=classes, activation='softmax')(merged)\n",
    "\n",
    "#MODEL\n",
    "\n",
    "vqa_model = tf.keras.Model(inputs=[cnn_input,encoder_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "CBGKVJNYT6Zp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, 400, 700, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_482 (Conv2D)             (None, 400, 700, 8)  1184        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 200, 350, 8)  0           conv2d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_474 (Activation)     (None, 200, 350, 8)  0           max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_483 (Conv2D)             (None, 200, 350, 16) 3216        activation_474[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 100, 175, 16) 0           conv2d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_475 (Activation)     (None, 100, 175, 16) 0           max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_484 (Conv2D)             (None, 100, 175, 32) 4640        activation_475[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 50, 87, 32)   0           conv2d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_485 (Conv2D)             (None, 50, 87, 32)   9248        max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 25, 43, 32)   0           conv2d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_476 (Activation)     (None, 25, 43, 32)   0           max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 21)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 34400)        0           activation_476[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 560000)       0           activation_474[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 21, 16)       74256       input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           2201664     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           17920032    flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           17920032    flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 128)          74240       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 128)          0           lstm_8[0][0]                     \n",
      "                                                                 concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 58)           7482        multiply_6[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 38,215,994\n",
      "Trainable params: 38,215,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vqa_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4Lw7k0pT6Zp"
   },
   "source": [
    "#### Optimization params to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "E_ihm4kvT6Zp"
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Validation metrics\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Compile Model\n",
    "vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model and add EarlyStopping to limit overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "AlbGCZQUT6Zp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "206/206 [==============================] - 97s 454ms/step - loss: 4.2576 - accuracy: 0.2859 - val_loss: 1.6286 - val_accuracy: 0.4484\n",
      "Epoch 2/100\n",
      "206/206 [==============================] - 92s 445ms/step - loss: 1.5979 - accuracy: 0.4492 - val_loss: 1.3395 - val_accuracy: 0.5340\n",
      "Epoch 3/100\n",
      "206/206 [==============================] - 92s 443ms/step - loss: 1.3757 - accuracy: 0.4888 - val_loss: 1.1810 - val_accuracy: 0.5435\n",
      "Epoch 4/100\n",
      "206/206 [==============================] - 92s 447ms/step - loss: 1.2364 - accuracy: 0.5327 - val_loss: 1.1554 - val_accuracy: 0.5149\n",
      "Epoch 5/100\n",
      "206/206 [==============================] - 91s 442ms/step - loss: 1.1842 - accuracy: 0.5356 - val_loss: 1.1062 - val_accuracy: 0.5802\n",
      "Epoch 6/100\n",
      "206/206 [==============================] - 92s 446ms/step - loss: 1.1499 - accuracy: 0.5525 - val_loss: 1.1194 - val_accuracy: 0.5693\n",
      "Epoch 7/100\n",
      "206/206 [==============================] - 113s 545ms/step - loss: 1.1293 - accuracy: 0.5715 - val_loss: 1.0589 - val_accuracy: 0.5829\n",
      "Epoch 8/100\n",
      "206/206 [==============================] - 115s 554ms/step - loss: 1.0977 - accuracy: 0.5794 - val_loss: 1.0580 - val_accuracy: 0.6060\n",
      "Epoch 9/100\n",
      "206/206 [==============================] - 92s 443ms/step - loss: 1.0331 - accuracy: 0.5941 - val_loss: 1.1044 - val_accuracy: 0.5639\n",
      "Epoch 10/100\n",
      "206/206 [==============================] - 92s 445ms/step - loss: 0.9064 - accuracy: 0.6575 - val_loss: 1.2221 - val_accuracy: 0.5394\n",
      "Epoch 11/100\n",
      "206/206 [==============================] - 91s 442ms/step - loss: 0.8804 - accuracy: 0.6702 - val_loss: 1.1151 - val_accuracy: 0.5693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb166fc1790>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = []\n",
    "\n",
    "# Early Stopping\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "vqa_model.fit(x=train_dataset,\n",
    "              validation_data=valid_dataset,\n",
    "              epochs=100,\n",
    "              steps_per_epoch=len(train_custom_dataset)/256,\n",
    "              validation_steps=len(valid_custom_dataset)/256,\n",
    "              batch_size=32, \n",
    "              callbacks=callbacks )\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTNnjv0GReP_"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "Rt_CPlXjWP2X"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "    with open(join(results_dir, csv_fname), 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6372"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(join(dataset_dir,'test_questions.json')) as json_file:\n",
    "    test_dict = json.load(json_file)\n",
    "\n",
    "# order json by 'image_id'    \n",
    "test_dict = dict(sorted(test_dict.items(), key=lambda x: int(x[1]['image_id'])))\n",
    "\n",
    "test_questions, predicted_category, question_ids = [],[],[]\n",
    "\n",
    "for line in test_dict.items():\n",
    "    \n",
    "    question_ids.append(line[0])\n",
    "    test_questions.append(line[1][\"question\"])\n",
    "    predicted_category.append(predict_category(line[1][\"question\"]))\n",
    "        \n",
    "filenames_test = [int(el[1]['image_id']) for el in test_dict.items()]   \n",
    "len(filenames_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tokenize Test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions: 1373\n",
      "Max question length: 18\n"
     ]
    }
   ],
   "source": [
    "# Create Tokenizer to convert words to integers\n",
    "test_questions_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "\n",
    "### pass the sentences retrieved from txt\n",
    "test_questions_tokenizer.fit_on_texts(test_questions)\n",
    "\n",
    "### returns a list of lists of indexes referring to words inside that sentence (max of words)\n",
    "test_questions_tokenized = test_questions_tokenizer.texts_to_sequences(test_questions)\n",
    "\n",
    "### returns a dict with words lowercased in alphabetical order and indexed wrt cardinality\n",
    "test_questions_wtoi = test_questions_tokenizer.word_index\n",
    "print('Questions:', len(test_questions_wtoi))\n",
    "\n",
    "max_test_question_length = max(len(sentence) for sentence in test_questions_tokenized)\n",
    "print('Max question length:', max_test_question_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad test questions to max training questions length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions encoder inputs shape: (6372, 21)\n"
     ]
    }
   ],
   "source": [
    "# Pad (with 0) to max question length\n",
    "questions_encoded_test = pad_sequences(test_questions_tokenized, maxlen=max_question_length)\n",
    "\n",
    "print(\"Questions encoder inputs shape:\", questions_encoded_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset modified for test purpose, returns tuple like ( [image, question], - )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "gm7nVJqnT6Zp"
   },
   "outputs": [],
   "source": [
    "class CustomDatasetTest(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, filenames, questions, preprocessing_function=preprocess_input, out_shape=[img_h,img_w],img_generator=ImageDataGenerator(rescale=1./255 - 0.5)):\n",
    "\n",
    "        self.questions = questions\n",
    "        self.filenames = filenames\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def __len__(self): return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Read Image\n",
    "        curr_filename = self.filenames[index]\n",
    "        img = Image.open(join(training_dir, str(curr_filename) + '.png'))\n",
    "        rgb_img = img.convert('RGB')\n",
    "\n",
    "        # Resize image\n",
    "        resized_img = rgb_img#.resize((img_h, img_w))\n",
    "        img_arr = np.array(resized_img)\n",
    "\n",
    "        # Array of 58 zeros, to match the needed generator\n",
    "        zero = np.zeros(shape=(58,))\n",
    "\n",
    "        return (img_arr,self.questions[index]), zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: (((None, 400, 700, 3), (None, 21)), (None, 58)), types: ((tf.uint8, tf.int32), tf.int32)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_custom_dataset = CustomDatasetTest ( filenames=filenames_test, questions=questions_encoded_test )\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator( lambda: test_custom_dataset,\n",
    "                                               output_types=((tf.uint8,tf.int32), tf.int32),\n",
    "                                               output_shapes=(([img_h, img_w, 3],(max_question_length,)), (58,)) )\n",
    "test_dataset = test_dataset.batch(32)\n",
    "test_dataset = test_dataset.repeat()\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2SFEk4tnG_L"
   },
   "source": [
    "#### Argmax based on category of the answer (explained in notebook \"1_data_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "B3aEC95VWlP_"
   },
   "outputs": [],
   "source": [
    "labels_indexes = [i for i in range(0,len(labels_dict))]\n",
    "\n",
    "def disactivate(category):\n",
    "    \n",
    "    if category==\"yes_no\":\n",
    "        return [item for item in labels_indexes if item not in (33,57)]\n",
    "    \n",
    "    elif category==\"counting\":\n",
    "        return [item for item in labels_indexes if item not in (0,1,2,3,4,5)]\n",
    "    \n",
    "    else:\n",
    "        return [0,1,2,3,4,5,33,57]\n",
    "    \n",
    "\n",
    "def weighted_argmax(prediction,k):\n",
    "   \n",
    "    #print(\"Before:  \\n\" + str(prediction) +\"\\n\")\n",
    "    \n",
    "    weighted_prediction = prediction\n",
    "    category = predicted_category[k]\n",
    "    to_be_disactivated = disactivate(category)\n",
    "   \n",
    "    #print(\"Category: \", category, \"->\", \"Putting to zero\",len(to_be_disactivated),\"elements\",\"\\n\")\n",
    "\n",
    "    for j in to_be_disactivated:\n",
    "        weighted_prediction[j] = 0\n",
    "   \n",
    "    #print(\"Later:  \\n\" + str(weighted_prediction) +\"\\n\")\n",
    "    \n",
    "    return np.argmax(weighted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vqa_model.predict( x=test_dataset, steps=len(test_custom_dataset)/32 )\n",
    "\n",
    "if len(predictions) == len(filenames_test):\n",
    "    print(\"Correctly created\",len(predictions),\"entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = random.randint(1,1000)\n",
    "\n",
    "# Image\n",
    "path = join(training_dir,str(filenames_test[n])+'.png')\n",
    "Image.open(path).resize((350,200), Image.NEAREST)\n",
    "\n",
    "# Question\n",
    "test_questions[n]\n",
    "\n",
    "# Predicted answer\n",
    "pr = weighted_argmax(predictions[n],n)\n",
    "switched_labels_dict = {int(y):x for x,y in labels_dict.items()}\n",
    "\"Predicted answer: \"+ switched_labels_dict[pr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually generating dict for testing evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxcVFHbm-rWH"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "results = {}\n",
    "\n",
    "for p in predictions:\n",
    "\n",
    "    prediction = weighted_argmax(p,i)    \n",
    "    results[question_ids[i]] = str(prediction)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbESxSHLsqy5"
   },
   "outputs": [],
   "source": [
    "create_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
